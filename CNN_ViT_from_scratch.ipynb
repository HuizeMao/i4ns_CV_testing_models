{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7eccd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2.9.1+cu128\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6c107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NumpyImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects:\n",
    "      X: (N, H, W, 3) uint8\n",
    "      y: (N,) int labels\n",
    "    \"\"\"\n",
    "    def __init__(self, x_path, y_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.images = np.load(x_path)   # (N, H, W, 3)\n",
    "        self.labels = np.load(y_path)   # (N,)\n",
    "        self.transform = transform\n",
    "\n",
    "        if self.images.ndim != 4 or self.images.shape[-1] != 3:\n",
    "            raise ValueError(f\"Expected images of shape (N, H, W, 3), got {self.images.shape}\")\n",
    "        if len(self.images) != len(self.labels):\n",
    "            raise ValueError(\"X and y must have the same length\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]          # numpy (H, W, 3), likely uint8\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        # to torch, CHW, float32 in [0, 1]\n",
    "        img = torch.from_numpy(img)     # (H, W, 3), dtype uint8 or similar\n",
    "        img = img.permute(2, 0, 1)      # (3, H, W)\n",
    "        img = img.float() / 255.0       # (3, H, W), float32\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8be604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloaders(\n",
    "    data_dir=\"dataset\",\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    shuffle_train=True,\n",
    "    transform=None,\n",
    "):\n",
    "    x_train_path = os.path.join(data_dir, \"x_train.npy\")\n",
    "    y_train_path = os.path.join(data_dir, \"y_train.npy\")\n",
    "    x_val_path   = os.path.join(data_dir, \"x_val.npy\")\n",
    "    y_val_path   = os.path.join(data_dir, \"y_val.npy\")\n",
    "    x_test_path  = os.path.join(data_dir, \"x_test.npy\")\n",
    "    y_test_path  = os.path.join(data_dir, \"y_test.npy\")\n",
    "\n",
    "    train_dataset = NumpyImageDataset(x_train_path, y_train_path, transform=transform)\n",
    "    val_dataset   = NumpyImageDataset(x_val_path,   y_val_path,   transform=transform)\n",
    "    test_dataset  = NumpyImageDataset(x_test_path,  y_test_path,  transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_dir=\"dataset\",\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad6c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 150, 150]) torch.float32\n",
      "torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape, images.dtype)   # (B, 3, H, W), torch.float32\n",
    "    print(labels.shape, labels.dtype)   # (B,), torch.int64\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8e41b",
   "metadata": {},
   "source": [
    "## CNN + ViT architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2582ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee406809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title refactored Pytorch version of CNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class turkey_cnn(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "    self.pool1 = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(64, 96, 3)\n",
    "    self.pool2 = nn.MaxPool2d(2, 2)\n",
    "    self.conv3 = nn.Conv2d(96, 96, 3)\n",
    "    self.pool3 = nn.MaxPool2d(2, 2)\n",
    "    self.pool4 = nn.MaxPool2d(2,2)\n",
    "  def forward(self, x):\n",
    "    x = self.pool1(F.relu(self.conv1(x)))\n",
    "    x = self.pool2(F.relu(self.conv2(x)))\n",
    "    x = self.pool3(F.relu(self.conv3(x)))\n",
    "    x = self.pool4(x)\n",
    "    return x\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# Assume 'YourModelClass' is the name of your PyTorch class\n",
    "cnn_model = turkey_cnn()\n",
    "\n",
    "# (Batch Size, Channels, Height, Width)\n",
    "input_shape = (100, 3, 150, 150)\n",
    "\n",
    "# Print the model summary\n",
    "# summary(cnn_model, input_size=input_shape)\n",
    "\n",
    "#@title ViT component using CNN as backbone\n",
    "\n",
    "# (B, C, H', W')\n",
    "\n",
    "class ViT_hm(nn.Module):\n",
    "  def __init__(self, cnn: turkey_cnn, C, D, N, num_heads, num_layers, num_cls):\n",
    "    super().__init__()\n",
    "    self.cnn = cnn\n",
    "    self.D = D\n",
    "    self.C = C\n",
    "    self.to_tokens = nn.Linear(in_features=C, out_features=D )\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, D))\n",
    "    self.pos_emb = nn.Parameter(torch.randn(1, N+1, D))\n",
    "\n",
    "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "        d_model = D,\n",
    "        nhead = num_heads,\n",
    "        batch_first = True # (B, N, D)\n",
    "    )\n",
    "    self.transformer = nn.TransformerEncoder(\n",
    "        encoder_layer = self.encoder_layer,\n",
    "        num_layers = num_layers\n",
    "    )\n",
    "    self.proj_cls = nn.Linear(D, num_cls)\n",
    "    self.norm = nn.LayerNorm(D)\n",
    "\n",
    "  def forward(self, x):\n",
    "    feat = self.cnn(x)\n",
    "    B, C, Hp, Wp = feat.shape\n",
    "    feat = feat.flatten(2) # (B, C, N), N=H'*W'\n",
    "    feat = feat.transpose(2, 1) # (B, N, C)\n",
    "    tokens = self.to_tokens(feat) # (B, N, D)\n",
    "    # cls token for global attending\n",
    "    cls = self.cls_token.expand(B, -1, -1)\n",
    "    x = torch.cat([cls, tokens], dim=1)\n",
    "\n",
    "    #add position embedding\n",
    "    x = x + self.pos_emb[:, : x.size(1),:]\n",
    "\n",
    "    # multiple passes of MHSA\n",
    "    x = self.transformer(x) # (B, N+1, D)\n",
    "    x = self.norm(x)\n",
    "    # extract cls and classify\n",
    "    cls_tok = x[:, 0, :] # (B, D)\n",
    "    logits = self.proj_cls(cls_tok) # (B, num_cls)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48bff7",
   "metadata": {},
   "source": [
    "### Define graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cafa59af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 32 * C 96 * Hp 8 * Wp 8\n",
      " B * N * D = 32 * 64 * 128\n",
      "ViT_hm(\n",
      "  (cnn): turkey_cnn(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (to_tokens): Linear(in_features=96, out_features=128, bias=True)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proj_cls): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_backbone = turkey_cnn()\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "first_batch_dat, first_batch_lab = next(data_iter)\n",
    "batch_size = first_batch_dat.size(0)\n",
    "feat = cnn_backbone(first_batch_dat)\n",
    "_, C, Hp, Wp = feat.shape\n",
    "N = Hp * Wp\n",
    "model = ViT_hm(cnn=cnn_backbone, C=C, D=128, N=N,\n",
    "               num_heads=4, num_layers=2, num_cls=2)\n",
    "\n",
    "print(f\"batch size: {batch_size} * C {C} * Hp {Hp} * Wp {Wp}\")\n",
    "print(f\" B * N * D = {batch_size} * {N} * {128}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ca1ba",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f46bdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Train loss: 0.7241, Train acc: 0.4960\n",
      "Epoch 1/25 - Val loss: 0.6934, Val acc: 0.4989\n",
      "Epoch 2/25 - Train loss: 0.7004, Train acc: 0.5028\n",
      "Epoch 2/25 - Val loss: 0.6904, Val acc: 0.4989\n",
      "Epoch 3/25 - Train loss: 0.6804, Train acc: 0.5524\n",
      "Epoch 3/25 - Val loss: 0.6536, Val acc: 0.5225\n",
      "Epoch 4/25 - Train loss: 0.5688, Train acc: 0.6981\n",
      "Epoch 4/25 - Val loss: 0.5065, Val acc: 0.7537\n",
      "Epoch 5/25 - Train loss: 0.5208, Train acc: 0.7520\n",
      "Epoch 5/25 - Val loss: 0.5185, Val acc: 0.7580\n",
      "Epoch 6/25 - Train loss: 0.4909, Train acc: 0.7728\n",
      "Epoch 6/25 - Val loss: 0.4366, Val acc: 0.7923\n",
      "Epoch 7/25 - Train loss: 0.4403, Train acc: 0.7949\n",
      "Epoch 7/25 - Val loss: 0.4777, Val acc: 0.7730\n",
      "Epoch 8/25 - Train loss: 0.4397, Train acc: 0.7906\n",
      "Epoch 8/25 - Val loss: 0.4185, Val acc: 0.8094\n",
      "Epoch 9/25 - Train loss: 0.4295, Train acc: 0.8169\n",
      "Epoch 9/25 - Val loss: 0.4173, Val acc: 0.8094\n",
      "Epoch 10/25 - Train loss: 0.4289, Train acc: 0.8047\n",
      "Epoch 10/25 - Val loss: 0.4385, Val acc: 0.8201\n",
      "Epoch 11/25 - Train loss: 0.4063, Train acc: 0.8273\n",
      "Epoch 11/25 - Val loss: 0.3968, Val acc: 0.8351\n",
      "Epoch 12/25 - Train loss: 0.3690, Train acc: 0.8414\n",
      "Epoch 12/25 - Val loss: 0.4069, Val acc: 0.8244\n",
      "Epoch 13/25 - Train loss: 0.3844, Train acc: 0.8365\n",
      "Epoch 13/25 - Val loss: 0.3819, Val acc: 0.8501\n",
      "Epoch 14/25 - Train loss: 0.3604, Train acc: 0.8518\n",
      "Epoch 14/25 - Val loss: 0.4537, Val acc: 0.7966\n",
      "Epoch 15/25 - Train loss: 0.4256, Train acc: 0.8077\n",
      "Epoch 15/25 - Val loss: 0.3887, Val acc: 0.8458\n",
      "Epoch 16/25 - Train loss: 0.3504, Train acc: 0.8432\n",
      "Epoch 16/25 - Val loss: 0.3899, Val acc: 0.8415\n",
      "Epoch 17/25 - Train loss: 0.3350, Train acc: 0.8585\n",
      "Epoch 17/25 - Val loss: 0.3701, Val acc: 0.8565\n",
      "Epoch 18/25 - Train loss: 0.3355, Train acc: 0.8592\n",
      "Epoch 18/25 - Val loss: 0.3583, Val acc: 0.8522\n",
      "Epoch 19/25 - Train loss: 0.3729, Train acc: 0.8451\n",
      "Epoch 19/25 - Val loss: 0.3728, Val acc: 0.8458\n",
      "Epoch 20/25 - Train loss: 0.3195, Train acc: 0.8671\n",
      "Epoch 20/25 - Val loss: 0.3459, Val acc: 0.8565\n",
      "Epoch 21/25 - Train loss: 0.3038, Train acc: 0.8690\n",
      "Epoch 21/25 - Val loss: 0.3440, Val acc: 0.8694\n",
      "Epoch 22/25 - Train loss: 0.3070, Train acc: 0.8775\n",
      "Epoch 22/25 - Val loss: 0.3699, Val acc: 0.8587\n",
      "Epoch 23/25 - Train loss: 0.2926, Train acc: 0.8732\n",
      "Epoch 23/25 - Val loss: 0.3648, Val acc: 0.8501\n",
      "Epoch 24/25 - Train loss: 0.2844, Train acc: 0.8861\n",
      "Epoch 24/25 - Val loss: 0.3472, Val acc: 0.8608\n",
      "Epoch 25/25 - Train loss: 0.2898, Train acc: 0.8806\n",
      "Epoch 25/25 - Val loss: 0.3152, Val acc: 0.8694\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ----- TRAIN -----\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    accuracy_train = 0.0\n",
    "\n",
    "    for img, lab in train_loader:\n",
    "        img = img.to(device)           # (B, 3, H, W), float32\n",
    "        lab = lab.to(device)           # (B,), long\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(img)               # (B, 2)\n",
    "        loss = loss_fn(out, lab)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss.item() * img.size(0)\n",
    "        _, preds = torch.max(out, 1)   # argmax over classes\n",
    "        accuracy_train += (preds == lab).sum().item()\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    epoch_acc_train = accuracy_train / len(train_loader.dataset)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "        f\"Train loss: {epoch_loss_train:.4f}, Train acc: {epoch_acc_train:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ----- VALIDATION -----\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    accuracy_val = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, lab in val_loader:\n",
    "            img = img.to(device)\n",
    "            lab = lab.to(device)\n",
    "\n",
    "            out = model(img)\n",
    "            loss = loss_fn(out, lab)\n",
    "\n",
    "            running_loss_val += loss.item() * img.size(0)\n",
    "            _, preds = torch.max(out, 1)\n",
    "            accuracy_val += (preds == lab).sum().item()\n",
    "\n",
    "    epoch_loss_val = running_loss_val / len(val_loader.dataset)\n",
    "    epoch_acc_val = accuracy_val / len(val_loader.dataset)\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "        f\"Val loss: {epoch_loss_val:.4f}, Val acc: {epoch_acc_val:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8c4d1",
   "metadata": {},
   "source": [
    "### evaluation with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3ebfa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3543, Test Accuracy: 0.8462, Test Precision: 0.8293, Test Recall: 0.8718, Test F1: 0.8500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, lab in test_loader:\n",
    "        img = img.to(device)\n",
    "        lab = lab.to(device)\n",
    "\n",
    "        out = model(img)                       # (B, 2)\n",
    "        loss = loss_fn(out, lab)\n",
    "\n",
    "        test_loss += loss.item() * img.size(0)\n",
    "\n",
    "        _, preds = torch.max(out, 1)           # (B,)\n",
    "        correct += (preds == lab).sum().item()\n",
    "        total += lab.size(0)\n",
    "\n",
    "        # Binary metrics, assuming positive class = 1\n",
    "        TP += ((preds == 1) & (lab == 1)).sum().item()\n",
    "        FP += ((preds == 1) & (lab == 0)).sum().item()\n",
    "        FN += ((preds == 0) & (lab == 1)).sum().item()\n",
    "\n",
    "# Averages\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = correct / total\n",
    "\n",
    "# Precision, recall, F1 for class 1\n",
    "if TP + FP > 0:\n",
    "    test_precision = TP / (TP + FP)\n",
    "else:\n",
    "    test_precision = 0.0\n",
    "\n",
    "if TP + FN > 0:\n",
    "    test_recall = TP / (TP + FN)\n",
    "else:\n",
    "    test_recall = 0.0\n",
    "\n",
    "if test_precision + test_recall > 0:\n",
    "    test_f1 = 2 * test_precision * test_recall / (test_precision + test_recall)\n",
    "else:\n",
    "    test_f1 = 0.0\n",
    "\n",
    "print(\n",
    "    f\"Test Loss: {test_loss:.4f}, \"\n",
    "    f\"Test Accuracy: {test_accuracy:.4f}, \"\n",
    "    f\"Test Precision: {test_precision:.4f}, \"\n",
    "    f\"Test Recall: {test_recall:.4f}, \"\n",
    "    f\"Test F1: {test_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec8d1c",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1df83cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript model to: cnn_vit_ship_classifier_jit.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def save_scripted_model(model, path=\"cnn_vit_ship_classifier_jit.pt\"):\n",
    "    model.eval()\n",
    "    cpu_model = model.to(\"cpu\")\n",
    "    scripted_model = torch.jit.script(cpu_model)\n",
    "    scripted_model.save(path)\n",
    "    print(f\"Saved TorchScript model to: {path}\")\n",
    "\n",
    "\n",
    "save_scripted_model(model, path=\"cnn_vit_ship_classifier_jit.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00c954",
   "metadata": {},
   "source": [
    "## Test on OOD Fire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19bf10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 61 OOD images in dataset/ood_on_fire\n",
      "OOD on fire evaluation (n=61):\n",
      "Accuracy : 0.6885\n",
      "Precision (military=1): 0.7143\n",
      "Recall    (military=1): 0.6452\n",
      "F1        (military=1): 0.6780\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ----------------------------\n",
    "# 1. OOD dataset for 150x150\n",
    "# ----------------------------\n",
    "\n",
    "class OODShipsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects a folder:\n",
    "        dataset/ood_on_fire/\n",
    "            civilian_....jpg / png / ...\n",
    "            military_....jpg / png / ...\n",
    "    Labels:\n",
    "        civilian_* -> 0\n",
    "        military_* -> 1\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".webp\"}\n",
    "\n",
    "        for fname in os.listdir(root):\n",
    "            fpath = os.path.join(root, fname)\n",
    "            if not os.path.isfile(fpath):\n",
    "                continue\n",
    "\n",
    "            ext = os.path.splitext(fname)[1].lower()\n",
    "            if ext not in exts:\n",
    "                continue\n",
    "\n",
    "            lower = fname.lower()\n",
    "            if lower.startswith(\"civilian\"):\n",
    "                label = 0\n",
    "            elif lower.startswith(\"military\"):\n",
    "                label = 1\n",
    "            else:\n",
    "                # skip files that don't match naming convention\n",
    "                continue\n",
    "\n",
    "            self.samples.append((fpath, label))\n",
    "\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No valid images found in {root}\")\n",
    "\n",
    "        print(f\"Found {len(self.samples)} OOD images in {root}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Transform + DataLoader\n",
    "# ----------------------------\n",
    "\n",
    "ood_root = \"dataset/ood_on_fire\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),  # <--- 150 x 150, as you requested\n",
    "    transforms.ToTensor(),          # (3, H, W), float32 in [0,1]\n",
    "])\n",
    "\n",
    "ood_dataset = OODShipsDataset(ood_root, transform=transform)\n",
    "\n",
    "ood_loader = DataLoader(\n",
    "    ood_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Inference + metrics\n",
    "# ----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labs in ood_loader:\n",
    "        imgs = imgs.to(device)          # (B, 3, 150, 150)\n",
    "        labs = labs.to(device)          # (B,)\n",
    "\n",
    "        logits = model(imgs)            # (B, 2)\n",
    "        preds = torch.argmax(logits, dim=1)  # (B,)\n",
    "\n",
    "        all_labels.extend(labs.cpu().numpy().tolist())\n",
    "        all_preds.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    average=\"binary\",\n",
    "    pos_label=1,  # military = 1\n",
    ")\n",
    "\n",
    "print(f\"OOD on fire evaluation (n={len(y_true)}):\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision (military=1): {precision:.4f}\")\n",
    "print(f\"Recall    (military=1): {recall:.4f}\")\n",
    "print(f\"F1        (military=1): {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
